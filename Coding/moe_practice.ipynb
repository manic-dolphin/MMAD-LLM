{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "        [-3.4028e+38,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38],\n",
      "        [-3.4028e+38, -3.4028e+38,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "sliding_window = 3\n",
    "input_ids = torch.randint(0, 10, (1, 5))\n",
    "dtype = torch.float\n",
    "bsz, tgt_len = 1, 5\n",
    "mask_cond = torch.arange(tgt_len)\n",
    "mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min)\n",
    "mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "print(mask)\n",
    "diagonal = 0 - sliding_window + 1\n",
    "context_mask = 1 - torch.triu(torch.ones_like(mask, dtype=torch.int), diagonal=diagonal)\n",
    "print(context_mask)\n",
    "mask.masked_fill_(context_mask, torch.finfo(dtype).min)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MixtralConfig = None\n",
    "import torch.nn as nn\n",
    "from transformers.activations import ACT2FN\n",
    "class MixtralBLockSparseTop2MLP(nn.Module):\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        super().__init__()\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)\n",
    "        current_hidden_states = self.w2(current_hidden_states)\n",
    "        return current_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MixtralSparseMoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This implementation is\n",
    "    strictly equivalent to standard MoE with full capacity (no\n",
    "    dropped tokens). It's faster since it formulates MoE operations\n",
    "    in terms of block-sparse operations to accomodate imbalanced\n",
    "    assignments of tokens to experts, whereas standard MoE either\n",
    "    (1) drop tokens at the cost of reduced performance or (2) set\n",
    "    capacity factor to number of experts and thus waste computation\n",
    "    and memory on padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "\n",
    "        # gating\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "\n",
    "        self.experts = nn.ModuleList([MixtralBLockSparseTop2MLP(config) for _ in range(self.num_experts)])\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \"\"\"\n",
    "        # 获取到每个token的mlp层输入特征 \n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        # router_logits: (batch * sequence_length, n_experts)\n",
    "        # 得到每个专家的打分，维度是batch * sequence, num_experts，取topk个专家\n",
    "        router_logits = self.gate(hidden_states)\n",
    "\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        # torch.topk是可导的\n",
    "        # (bs * sl, topk)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        # 取到topk个专家的打分，需要归一化，用于对后面的expert计算出来的结果进行加权\n",
    "        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        # we cast back to the input dtype\n",
    "        routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask\n",
    "        # this will be used to easily index which expert is going to be sollicitated\n",
    "        # (bs * sl, topk, n_experts) -> (n_experts, topk, bs * sl);\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # 这样取到expert_mask[expert_idx]，从上面的注释可以知道维度是[topk, bs * sl]\n",
    "            # torch.where的结果，第一个结果代表选到了哪一行，第二个代表选择了哪一列\n",
    "            # 对应到实际意义，top_x表示取的列，也就是取哪些token\n",
    "            # 而行表示，取到的这些token，根据路由gate计算，当前expert是排行第几\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            if top_x.shape[0] == 0:\n",
    "            # 没有token需要当前的expert计算\n",
    "                continue\n",
    "\n",
    "            # in torch it is faster to index using lists than torch tensors\n",
    "            top_x_list = top_x.tolist()\n",
    "            idx_list = idx.tolist()\n",
    "\n",
    "            # Index the correct hidden states and compute the expert hidden state for\n",
    "            # the current expert. We need to make sure to multiply the output hidden\n",
    "            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
    "            # 前面hidden states已经转成了 [bs * sl, hs]，根据top_x可以找到需要计算的token\n",
    "            current_state = hidden_states[None, top_x_list].reshape(-1, hidden_dim)\n",
    "            # 找到这个expert对应的权重 乘进去\n",
    "            # 上面计算的权重是routing_weights，维度是bs * sl, topk\n",
    "            # 根据top_x_list 对应的token，idx_list表示topk中第几个可以直接取到相应的权重\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list, None]\n",
    "\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use\n",
    "            # the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        return final_hidden_states, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 2],\n",
      "        [5, 0],\n",
      "        [5, 4],\n",
      "        [6, 4],\n",
      "        [6, 5]])\n",
      "tensor([[[0, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 1, 0]],\n",
      "\n",
      "        [[1, 1, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 1, 1],\n",
      "         [0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0]]])\n",
      "torch.Size([8, 2, 5])\n",
      "tensor([[[0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0, 0]]])\n",
      "(tensor([], dtype=torch.int64), tensor([], dtype=torch.int64))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "selected_experts = torch.randint(0, 8, (5, 2))\n",
    "print(selected_experts)\n",
    "expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=8).permute(2, 1, 0)\n",
    "print(expert_mask)\n",
    "print(expert_mask.shape)\n",
    "print(torch.nn.functional.one_hot(selected_experts, num_classes=8))\n",
    "print(torch.where(expert_mask[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
